{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9470b88fcd1467c0",
   "metadata": {},
   "source": [
    "# PGD Attack with TensorBoard Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19927fcb03f7c5f9",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate how to use SecML-Torch to perform a Projected Gradient Descent (PGD) attack while tracking various metrics using TensorBoard integration.\n",
    "\n",
    "We will:\n",
    "- Load visualization utilities and dependencies\n",
    "- Load the CIFAR-10 dataset and a pre-trained robust model\n",
    "- Configure a PGD attack with multiple tracking capabilities\n",
    "- Visualize the attack results and tracked metrics using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60c2f1b9ecfe9d",
   "metadata": {},
   "source": [
    "#### Import dependencies and load utils functions\n",
    "\n",
    "We install/load SecMLâ€‘Torch, RobustBench and TensorBoard (to visualize metrics). If this is your first run, packages and the CIFARâ€‘10 dataset may be downloaded, which requires internet access."
   ]
  },
  {
   "cell_type": "code",
   "id": "5847d122dc944ba6",
   "metadata": {},
   "source": [
    "%%capture --no-stderr --no-stdout\n",
    "try:\n",
    "    import secmlt\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/pralab/secml-torch\n",
    "\n",
    "try:\n",
    "    import tensorboard\n",
    "except ImportError:\n",
    "    %pip install tensorboard\n",
    "\n",
    "try:\n",
    "    import robustbench\n",
    "except ImportError:\n",
    "    %pip install robustbench"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a1cecb03d662493",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# SecML-Torch imports\n",
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.metrics.classification import Accuracy\n",
    "from secmlt.adv.evasion.pgd import PGD\n",
    "from secmlt.adv.evasion.perturbation_models import LpPerturbationModels\n",
    "from secmlt.adv.backends import Backends\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# CIFAR-10 class names\n",
    "cifar10_classes = [\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "\n",
    "def convert_cifar_image(image):\n",
    "    \"\"\"Convert tensor image to numpy format for matplotlib display.\"\"\"\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().numpy()\n",
    "    if image.ndim == 3:\n",
    "        image = image.transpose((1, 2, 0))  # CHW to HWC\n",
    "    # Ensure values are in [0, 1] range for display\n",
    "    image = np.clip(image, 0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def show_adversarial_comparison(\n",
    "    clean_samples,\n",
    "    adv_samples,\n",
    "    clean_labels,\n",
    "    adv_preds,\n",
    "    title=\"Clean vs Adversarial Examples\",\n",
    "    n_display=5,\n",
    "):\n",
    "    \"\"\"Display a 3-row grid of clean images, adversarial images, and their perturbations (scaled Ã—100), showing true labels, predicted labels (color-coded), and the Lâˆž norm of the unscaled perturbation.\"\"\"\n",
    "    n_display = min(n_display, len(clean_samples))\n",
    "    fig, axes = plt.subplots(3, n_display, figsize=(2.5 * n_display, 8))\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    for idx in range(n_display):\n",
    "        # Clean image\n",
    "        clean_img = convert_cifar_image(clean_samples[idx])\n",
    "        axes[0, idx].imshow(clean_img)\n",
    "        if idx == 0:\n",
    "            axes[0, idx].set_ylabel(\"Clean\", fontsize=12)\n",
    "        axes[0, idx].set_title(\n",
    "            f\"True: {cifar10_classes[clean_labels[idx]]} ({clean_labels[idx]})\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "        axes[0, idx].axis(\"off\")\n",
    "\n",
    "        # Adversarial image\n",
    "        adv_img = convert_cifar_image(adv_samples[idx])\n",
    "        axes[1, idx].imshow(adv_img)\n",
    "        if idx == 0:\n",
    "            axes[1, idx].set_ylabel(\"Adversarial\", fontsize=12)\n",
    "        color = \"green\" if clean_labels[idx] == adv_preds[idx] else \"red\"\n",
    "        axes[1, idx].set_title(\n",
    "            f\"Pred: {cifar10_classes[adv_preds[idx]]} ({adv_preds[idx]})\",\n",
    "            color=color,\n",
    "            fontsize=10,\n",
    "        )\n",
    "        axes[1, idx].axis(\"off\")\n",
    "\n",
    "        # Perturbation (scaled for visibility)\n",
    "        perturbation = adv_samples[idx] - clean_samples[idx]\n",
    "        scaled_perturbation = perturbation * 100\n",
    "        pert_img = convert_cifar_image(scaled_perturbation)\n",
    "        axes[2, idx].imshow(pert_img)\n",
    "        if idx == 0:\n",
    "            axes[2, idx].set_ylabel(\"Perturbation\", fontsize=12)\n",
    "\n",
    "        # Calculate Lâˆž norm\n",
    "        linf_norm = torch.norm(perturbation.view(-1), p=float(\"inf\")).item()\n",
    "        axes[2, idx].set_title(f\"Lâˆž: {linf_norm:.4f} (||Î´|| Ã— 100)\", fontsize=10)\n",
    "        axes[2, idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "251155f1fa189067",
   "metadata": {},
   "source": [
    "#### Set device and paths"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d4b33a86bf05121",
   "metadata": {},
   "source": [
    "# Setup device and paths\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_path = \"data/datasets/\"\n",
    "logs_path = \"data/logs/pgd_tutorial\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(logs_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "print(f\"Logs will be saved to: {logs_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f80764baad853557",
   "metadata": {},
   "source": [
    "#### Loading CIFAR-10 Dataset\n",
    "\n",
    "We'll load the CIFAR-10 dataset and use a small subset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "id": "261a2f37e6143e08",
   "metadata": {},
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=dataset_path, train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "num_samples = 20\n",
    "batch_size = num_samples // 2\n",
    "test_subset = Subset(test_dataset, list(range(num_samples)))\n",
    "test_data_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Loaded {len(test_subset)} samples from CIFAR-10 test set\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c40818c7be33a7a",
   "metadata": {},
   "source": [
    "#### Loading a Pre-trained Robust Model\n",
    "\n",
    "We'll load a robust model from RobustBench that has been trained to withstand adversarial attacks. We are loading a model that has been adversarially trained against Linf norm adversarial attacks (specified by the \"threat_model\" field)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1cef202d03f1cb1",
   "metadata": {},
   "source": [
    "from robustbench.utils import load_model\n",
    "\n",
    "# Load a robust model from RobustBench\n",
    "net = load_model(model_name=\"Rony2019Decoupling\", dataset=\"cifar10\", threat_model=\"L2\")\n",
    "net = net.to(device)\n",
    "net.eval()\n",
    "\n",
    "# Wrap the model with SecML-Torch's BasePytorchClassifier\n",
    "model = BasePytorchClassifier(net)\n",
    "\n",
    "print(f\"Loaded robust model: Rony2019Decoupling\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c784ea903537f6c8",
   "metadata": {},
   "source": [
    "#### Baseline Performance Evaluation\n",
    "\n",
    "Let's evaluate the model's performance on clean images. We'll later use this information to compare performance degradation of the model, after computing adversarial examples through PGD attack."
   ]
  },
  {
   "cell_type": "code",
   "id": "e52e522dd87001ec",
   "metadata": {},
   "source": [
    "# Test accuracy on clean examples\n",
    "clean_accuracy = Accuracy()(model, test_data_loader)\n",
    "print(\n",
    "    f\"Clean accuracy: {clean_accuracy.item():.4f} ({clean_accuracy.item() * 100:.2f}%)\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b85a6059543c9dfc",
   "metadata": {},
   "source": [
    "#### Configuring PGD Attack with Tracking\n",
    "\n",
    "Here we import per-step trackers for loss, predictions, perturbation norm (Lâˆž), and gradient norm, plus the TensorBoard tracker that aggregates these signals and logs them to `logs_path` for visualization. Then we configure PGD hyperparameters and attach the tracker so that every iteration is recorded and viewable in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "id": "d76f177c16789190",
   "metadata": {},
   "source": [
    "# Import tracking components\n",
    "from secmlt.trackers import (\n",
    "    LossTracker,\n",
    "    PredictionTracker,\n",
    "    PerturbationNormTracker,\n",
    "    GradientNormTracker,\n",
    "    TensorboardTracker,\n",
    ")\n",
    "\n",
    "print(\"Tracking components imported successfully\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89789e8f41de3a68",
   "metadata": {},
   "source": [
    "# Configure PGD attack parameters\n",
    "epsilon = 8 / 255     # Maximum Lâˆž perturbation\n",
    "num_steps = 10        # Number of PGD iterations\n",
    "step_size = 4 / 255   # Step size per iteration\n",
    "perturbation_model = LpPerturbationModels.LINF  # Lâˆž norm constraint\n",
    "\n",
    "print(f\"Attack configuration:\")\n",
    "print(f\"  - Epsilon: {epsilon:.4f} ({epsilon * 255:.1f}/255)\")\n",
    "print(f\"  - Number of steps: {num_steps}\")\n",
    "print(f\"  - Step size: {step_size:.4f} ({step_size * 255:.1f}/255)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e30f42795cde38c",
   "metadata": {},
   "source": [
    "# Set up individual trackers\n",
    "trackers = [\n",
    "    LossTracker(),\n",
    "    PredictionTracker(),\n",
    "    PerturbationNormTracker(\"linf\"),\n",
    "    GradientNormTracker(),\n",
    "]\n",
    "\n",
    "# Set up TensorBoard tracking\n",
    "tensorboard_tracker = TensorboardTracker(logs_path, trackers)\n",
    "\n",
    "print(f\"Configured {len(trackers)} trackers with TensorBoard logging\")\n",
    "print(f\"TensorBoard logs cleared and will be saved to: {logs_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac267ddb6888088b",
   "metadata": {},
   "source": [
    "#### Executing the PGD Attack with Tracking\n",
    "\n",
    "We instantiate PGD with the chosen hyperparameters and attach the TensorBoard tracker, then run it on the test dataloader. While the attack iterates, the trackers record per-step loss, predictions, perturbation Lâˆž norms, and gradient norms; these are written to `logs_path` for inspection in TensorBoard. The call returns an adversarial dataset aligned with the input batches, which we use for evaluation and visualization."
   ]
  },
  {
   "cell_type": "code",
   "id": "d16066d4c191584e",
   "metadata": {},
   "source": [
    "# Create PGD attack with tracking\n",
    "pgd_attack = PGD(\n",
    "    perturbation_model=perturbation_model,\n",
    "    epsilon=epsilon,\n",
    "    num_steps=num_steps,\n",
    "    step_size=step_size,\n",
    "    random_start=False,\n",
    "    backend=Backends.NATIVE,\n",
    "    trackers=tensorboard_tracker,\n",
    ")\n",
    "\n",
    "print(\"PGD attack configured successfully\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2361152967240048",
   "metadata": {},
   "source": [
    "# Execute the attack\n",
    "print(\"Executing PGD attack with tracking...\")\n",
    "pgd_native_adv_ds = pgd_attack(model, test_data_loader)\n",
    "print(\"Attack completed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7abca9ffbb6f41c0",
   "metadata": {},
   "source": [
    "#### Results Analysis\n",
    "\n",
    "Let's analyze the results of our PGD attack. We expect here to see performance degradation in terms of accuracy, which is the effect of the untargeted PGD attack we just run on our samples."
   ]
  },
  {
   "cell_type": "code",
   "id": "7b4b44ec907eac26",
   "metadata": {},
   "source": [
    "# Test accuracy on adversarial examples\n",
    "adversarial_accuracy = Accuracy()(model, pgd_native_adv_ds)\n",
    "\n",
    "print(\"=== Attack Results ===\")\n",
    "print(\n",
    "    f\"Clean accuracy:        {clean_accuracy.item():.4f} ({clean_accuracy.item() * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Adversarial accuracy:  {adversarial_accuracy.item():.4f} ({adversarial_accuracy.item() * 100:.2f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Attack success rate:   {1 - adversarial_accuracy.item():.4f} ({(1 - adversarial_accuracy.item()) * 100:.2f}%)\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b92036071b7c0d50",
   "metadata": {},
   "source": [
    "# Visualize clean vs adversarial examples\n",
    "print(\"Clean vs adversarial examples (the perturbation is amplified):\")\n",
    "\n",
    "# Get samples for visualization\n",
    "clean_iter = iter(test_data_loader)\n",
    "clean_images, clean_labels = next(clean_iter)\n",
    "\n",
    "adv_iter = iter(pgd_native_adv_ds)\n",
    "adv_images, adv_labels = next(adv_iter)\n",
    "\n",
    "# Get predictions\n",
    "clean_preds = model.predict(clean_images.to(device))\n",
    "adv_preds = model.predict(adv_images.to(device))\n",
    "\n",
    "# Show comparison\n",
    "show_adversarial_comparison(\n",
    "    clean_images,\n",
    "    adv_images,\n",
    "    clean_labels,\n",
    "    adv_preds,\n",
    "    title=\"Clean vs Adversarial Examples\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "raw-tracker-explain",
   "metadata": {},
   "source": [
    "#### Accessing Raw Tracker Metrics\n",
    "\n",
    "Beyond TensorBoard, you can inspect trackers directly in code. Each tracker collects per-sample values at each PGD iteration and exposes a `get()` method that returns a tensor with shape `[num_samples, num_iterations]` (concatenated across batches). The example below uses `LossTracker` and `PredictionTracker`. We compute prediction flips, where 1 means a flip in the true label happened, which should match the display above."
   ]
  },
  {
   "cell_type": "code",
   "id": "raw-tracker-code",
   "metadata": {},
   "source": [
    "# Inspect tracker histories\n",
    "loss_hist = trackers[0].get()   # LossTracker: shape [N_samples, T_iters]\n",
    "preds_hist = trackers[1].get()  # PredictionTracker: shape [N_samples, T_iters]\n",
    "\n",
    "# Average loss over iterations\n",
    "avg_loss = loss_hist.mean(dim=0) if loss_hist.numel() > 0 else None\n",
    "\n",
    "# Prediction changes across iterations per sample\n",
    "if preds_hist.numel() > 0:\n",
    "    pred_flips_per_sample = (preds_hist[:, 1:] != preds_hist[:, :-1]).sum(dim=1)\n",
    "    total_flips = int(pred_flips_per_sample.sum().item())\n",
    "else:\n",
    "    pred_flips_per_sample, total_flips = None, 0\n",
    "\n",
    "print(\"Raw tracker summaries:\")\n",
    "if avg_loss is not None:\n",
    "    print(f\" - Final average loss (iter {avg_loss.numel()-1}): {avg_loss[-1].item():.4f}\")\n",
    "else:\n",
    "    print(\" - Loss history empty\")\n",
    "if pred_flips_per_sample is not None:\n",
    "    print(f\" - Total prediction flips across samples/iters: {total_flips}\")\n",
    "    print(f\" - First 5 samples flips: {pred_flips_per_sample[:5].tolist()}\")\n",
    "else:\n",
    "    print(\" - Prediction history empty\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca1e9bbdbea66580",
   "metadata": {},
   "source": [
    "#### TensorBoard Visualization\n",
    "\n",
    "Now let's launch TensorBoard to see all the tracked metrics in detail. TensorBoard provides the most comprehensive view of the attack progression with interactive plots. We can view it inline (uncomment line below print statement), or access to the webpage on localhost. We can see how the loss, norms values progress during the attack steps."
   ]
  },
  {
   "cell_type": "code",
   "id": "2de26b9e9a2b3180",
   "metadata": {},
   "source": [
    "# Launch TensorBoard inline\n",
    "print(\n",
    "    \"ðŸš€ TensorBoard: http://localhost:6007 (open in your browser if not shown inline)\\n\"\n",
    ")\n",
    "\n",
    "# Uncomment this for inline visualization\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir $logs_path --port 6007 --reload_interval 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0ebc10bd13987d",
   "metadata": {},
   "source": [
    "![04-pgd-tensorboard-tracking.png](../_static/assets/tutorials/04-pgd-tensorboard-tracking.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
